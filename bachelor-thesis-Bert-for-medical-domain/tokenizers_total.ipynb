{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0d36244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import transformers\n",
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "from tokenizers import SentencePieceUnigramTokenizer, ByteLevelBPETokenizer\n",
    "from typing import List\n",
    "from transformers import BertTokenizer ## to create the config files for tokenizer wordpiece only\n",
    "from transformers import RobertaTokenizer ## to create the config for sentence piece/ BPE\n",
    "import os\n",
    "import string\n",
    "from typing import List\n",
    "import glob\n",
    "import re\n",
    "from data_prep_utils import get_txt_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9446b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\\nlist_of_files = get_txt_from_dir(path_to_data_files)\\nsave_path = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/BPE_tokenizer_dropout\"\\nfor i in [1000, 1200, 2000,4000]:#[1500,1800,2000,4000, 8000, 16000]:\\n    print(i)\\n    bpe_tokenizer_trainer(list_of_files, i,\\n                          save_path, save_model=True,\\n                    dropout = 0) '"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bpe_tokenizer_trainer(\n",
    "                list_of_files : list[str],\n",
    "                vocab_size : int,\n",
    "                save_path : str,\n",
    "                min_frequency : int = 2,\n",
    "                special_tokens : list[str] = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                model_max_length : int = 512,\n",
    "                show_progress : bool = True,\n",
    "                save_model=False,\n",
    "                dropout = 0\n",
    "                ):\n",
    "    if (dropout != 0):\n",
    "        bpe = ByteLevelBPETokenizer(dropout=dropout)\n",
    "    else:\n",
    "        bpe = ByteLevelBPETokenizer()\n",
    "        \n",
    "    bpe.train_from_iterator(\n",
    "        list_of_files,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        show_progress=show_progress,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "\n",
    "    bpe_fast = transformers.PreTrainedTokenizerFast(tokenizer_object=bpe, model_max_length=model_max_length,\n",
    "                special_tokens=special_tokens)\n",
    "\n",
    "    bpe.unk_token_=\"[UNK]\"\n",
    "    bpe.sep_token_=\"[SEP]\"\n",
    "    bpe.pad_token_=\"[PAD]\"\n",
    "    bpe.cls_token_=\"[CLS]\"\n",
    "    bpe.mask_token=\"[MASK]\"\n",
    "\n",
    "\n",
    "    bpe_fast.pad_token_id = bpe.token_to_id(\"[PAD]\")\n",
    "    bpe_fast.unk_token_id = bpe.token_to_id(\"[UNK]\")\n",
    "    bpe_fast.cls_token_id = bpe.token_to_id(\"[CLS]\")\n",
    "    bpe_fast.sep_token_id = bpe.token_to_id(\"[SEP]\")\n",
    "    bpe_fast.mask_token_id = bpe.token_to_id(\"[MASK]\")\n",
    "    \n",
    "    if save_model:\n",
    "        bpe_fast.save_pretrained(f\"{save_path}/1_BPE_1_batch_size_8_vocab_{vocab_size}_not\")\n",
    "    \n",
    "    return bpe_fast\n",
    "\"\"\"\n",
    "path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\n",
    "list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "save_path = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/BPE_tokenizer_dropout\"\n",
    "for i in [1000, 1200, 2000,4000]:#[1500,1800,2000,4000, 8000, 16000]:\n",
    "    print(i)\n",
    "    bpe_tokenizer_trainer(list_of_files, i,\n",
    "                          save_path, save_model=True,\n",
    "                    dropout = 0) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58ba1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjust with RobertaTokenizer\n",
    "def sentencepiece_tk_unigram(data : List[str], vocab_size : int, save_path : str,\n",
    "                 special_tokens : list = None, min_frequence=2, show_progress=True,\n",
    "                 model_max_length : int = 512):\n",
    "    if (special_tokens == None):\n",
    "        special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]\n",
    "\n",
    "    tk_tokenizer = SentencePieceUnigramTokenizer()\n",
    "    tk_tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        show_progress=show_progress,\n",
    "        special_tokens=special_tokens,\n",
    "        unk_token = \"<unk>\"\n",
    "    )\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    tk_tokenizer.save(save_path + \"/tokenizer.json\")\n",
    "    # convert\n",
    "    tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tk_tokenizer, model_max_length=model_max_length, special_tokens=special_tokens)\n",
    "    tokenizer.bos_token = \"<s>\"\n",
    "    tokenizer.bos_token_id = tk_tokenizer.token_to_id(\"<s>\")\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "    tokenizer.pad_token_id = tk_tokenizer.token_to_id(\"<pad>\")\n",
    "    tokenizer.eos_token = \"</s>\"\n",
    "    tokenizer.eos_token_id = tk_tokenizer.token_to_id(\"</s>\")\n",
    "    tokenizer.unk_token = \"<unk>\"\n",
    "    tokenizer.unk_token_id = tk_tokenizer.token_to_id(\"<unk>\")\n",
    "    tokenizer.cls_token = \"<cls>\"\n",
    "    tokenizer.cls_token_id = tk_tokenizer.token_to_id(\"<cls>\")\n",
    "    tokenizer.sep_token = \"<sep>\"\n",
    "    tokenizer.sep_token_id = tk_tokenizer.token_to_id(\"<sep>\")\n",
    "    tokenizer.mask_token = \"<mask>\"\n",
    "    tokenizer.mask_token_id = tk_tokenizer.token_to_id(\"<mask>\")\n",
    "\n",
    "    # and save for later!\n",
    "    tokenizer.save_pretrained(save_path)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6233c724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    tokenizer = RobertaTokenizer(vocab_file = save_path,\\n                tokenizer_file=tokenizer_file,\\n                errors=errors,\\n                bos_token=bos_token,\\n                eos_token=eos_token,\\n                sep_token=sep_token,\\n                cls_token=cls_token,\\n                unk_token=unk_token,\\n                pad_token=pad_token,\\n                mask_token=mask_token,\\n                add_prefix_space=add_prefix_space,\\n                trim_offsets=trim_offsets,\\n                model_max_length=model_max_length)\\n    \\n   '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## adjust with RobertaTokenizer\n",
    "def sentencepiece_tk(data : List[str], vocab_size : int, save_path : str,\n",
    "                 special_tokens : list = None, min_frequence=2, show_progress=True,\n",
    "                 model_max_length : int = 512, dropout=0):\n",
    "    if (special_tokens == None):\n",
    "        special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]\n",
    "    \n",
    "    if(dropout != 0):\n",
    "        \n",
    "        tk_tokenizer = SentencePieceBPETokenizer(dropout = dropout)\n",
    "    else:\n",
    "        tk_tokenizer = SentencePieceBPETokenizer()\n",
    "        \n",
    "    tk_tokenizer.train_from_iterator(\n",
    "        data,\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequence,\n",
    "        show_progress=show_progress,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    tk_tokenizer.save(save_path + \"/tokenizer.json\")\n",
    "    # convert\n",
    "    tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tk_tokenizer, model_max_length=model_max_length, special_tokens=special_tokens)\n",
    "    tokenizer.bos_token = \"<s>\"\n",
    "    tokenizer.bos_token_id = tk_tokenizer.token_to_id(\"<s>\")\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "    tokenizer.pad_token_id = tk_tokenizer.token_to_id(\"<pad>\")\n",
    "    tokenizer.eos_token = \"</s>\"\n",
    "    tokenizer.eos_token_id = tk_tokenizer.token_to_id(\"</s>\")\n",
    "    tokenizer.unk_token = \"<unk>\"\n",
    "    tokenizer.unk_token_id = tk_tokenizer.token_to_id(\"<unk>\")\n",
    "    tokenizer.cls_token = \"<cls>\"\n",
    "    tokenizer.cls_token_id = tk_tokenizer.token_to_id(\"<cls>\")\n",
    "    tokenizer.sep_token = \"<sep>\"\n",
    "    tokenizer.sep_token_id = tk_tokenizer.token_to_id(\"<sep>\")\n",
    "    tokenizer.mask_token = \"<mask>\"\n",
    "    tokenizer.mask_token_id = tk_tokenizer.token_to_id(\"<mask>\")\n",
    "\n",
    "    # and save for later!\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    return tokenizer\n",
    "\"\"\"\n",
    "    tokenizer = RobertaTokenizer(vocab_file = save_path,\n",
    "                tokenizer_file=tokenizer_file,\n",
    "                errors=errors,\n",
    "                bos_token=bos_token,\n",
    "                eos_token=eos_token,\n",
    "                sep_token=sep_token,\n",
    "                cls_token=cls_token,\n",
    "                unk_token=unk_token,\n",
    "                pad_token=pad_token,\n",
    "                mask_token=mask_token,\n",
    "                add_prefix_space=add_prefix_space,\n",
    "                trim_offsets=trim_offsets,\n",
    "                model_max_length=model_max_length)\n",
    "    \n",
    "   \"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7a11c0cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "finished training with  1000  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  1200  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  1500  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  1800  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  2000  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  4000  vocab size\n",
      "\n",
      "\n",
      "\n",
      "finished training with  8000  vocab size\n"
     ]
    }
   ],
   "source": [
    "path_to_txt = \"/home/fb198/BA/DataNephroTexts/train_tokenizer_data/numbers_punt_filter\"\n",
    "list_of_files = get_txt_from_dir(path_to_txt)\n",
    "for i in [1000, 1200, 1500, 1800, 2000, 4000, 8000]:# , \n",
    "    save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/tokenizers/sentencepiece_filtered_data/tokenizer_sp_duplicated/sentencepiece_vocab_{i}/\"\n",
    "    sentencepiece_tk(data=list_of_files, vocab_size=i, save_path=save_path, dropout=0.1)\n",
    "    print(\"finished training with \", i, \" vocab size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b20dc4",
   "metadata": {},
   "source": [
    "   bertTokenizer = BertTokenizer(vocab_file=save_path+\"/tokenizer.json\",\n",
    "        do_lower_case=False,\n",
    "        do_basic_tokenize=False,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        strip_accents=None, \n",
    "        max_model_length=512)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed049c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece_tokenizer(files : list[str], vocab_size : int, save_path : str,\n",
    "                        min_frequency=2, special_tokens : list[str] = None, max_length = 512,\n",
    "                        show_progress = True):\n",
    "    tk_tokenizer = BertWordPieceTokenizer()\n",
    "    print(\"about to train\")\n",
    "\n",
    "    if(special_tokens == None):\n",
    "        tk_tokenizer.train(files = files, vocab_size = vocab_size, min_frequency=min_frequency)\n",
    "    else:\n",
    "        tk_tokenizer.train(files = files, vocab_size = vocab_size,\n",
    "                           min_frequency=min_frequency, special_tokens=special_tokens,\n",
    "                           show_progress=show_progress)\n",
    "    tk_tokenizer.enable_truncation(max_length=max_length) \n",
    "    print(\"done training.. \\n\")\n",
    "    tk_tokenizer.save(save_path+\"tokenizer.json\")\n",
    "\n",
    "    wp_fast = transformers.PreTrainedTokenizerFast(tokenizer_object=tk_tokenizer, model_max_length=512,\n",
    "                special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "    bpe.unk_token_=\"[UNK]\"\n",
    "    bpe.sep_token_=\"[SEP]\"\n",
    "    bpe.pad_token_=\"[PAD]\"\n",
    "    bpe.cls_token_=\"[CLS]\"\n",
    "    bpe.mask_token=\"[MASK]\"\n",
    "\n",
    "\n",
    "    wp_fast.pad_token_id = tk_tokenizer.token_to_id(\"[PAD]\")\n",
    "    wp_fast.unk_token_id = tk_tokenizer.token_to_id(\"[UNK]\")\n",
    "    wp_fast.cls_token_id = tk_tokenizer.token_to_id(\"[CLS]\")\n",
    "    wp_fast.sep_token_id = tk_tokenizer.token_to_id(\"[SEP]\")\n",
    "    wp_fast.mask_token_id = tk_tokenizer.token_to_id(\"[MASK]\")    \n",
    "\n",
    "    wp_fast.save_pretrained(save_path)    # and save for later!\n",
    "    #tokenizer.save_pretrained(save_path)\n",
    "    #return bertTokenizer\n",
    "    \n",
    "#save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/tokenizer_WordPiece/Wordpiece_vocab_3000_temp\"\n",
    "#tt = wordpiece_tokenizer(files = list_of_files, vocab_size = 3000,\n",
    "#                    save_path = save_path, min_frequency=2)\n",
    "#print(\"done with the first word piece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71309cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordpiece_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_660834/2942796303.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/tokenizer_WordPiece/Wordpiece_vocab_860\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     wordpiece_tokenizer(files = list_of_files, vocab_size = i,\n\u001b[0m\u001b[1;32m      8\u001b[0m                         save_path = save_path, min_frequency=2)\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done with the first word piece\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wordpiece_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\n",
    "list_of_files = get_txt_from_dir(get_txt_from_dir)\n",
    "\n",
    "for i in [860]:# , \n",
    "    save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/tokenizer_WordPiece/Wordpiece_vocab_860\"\n",
    "\n",
    "    wordpiece_tokenizer(files = list_of_files, vocab_size = i,\n",
    "                        save_path = save_path, min_frequency=2)\n",
    "    print(\"done with the first word piece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0998fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    i = 3\n",
    "    \n",
    "    if(i == 0):\n",
    "        path_file = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/prepare_data/sentences_for_tokenizer_duplicated.txt\"\n",
    "        with open(path_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            data_dup = f.read()\n",
    "        data_dup = data_dup.split(\"\\n\")\n",
    "\n",
    "        for i in [1000, 1500, 1800, 2000, 2500, 3000, 4000, 8000,16000, 32000]:# , \n",
    "            save_path = f\"tokenizer_sp_duplicated/sentence_piece_vocab_{i}/\"\n",
    "            sentencepiece_tk(data=data_dup, vocab_size=i, save_path=save_path)\n",
    "            print(\"finished training with \", i, \" vocab size\")\n",
    "            \n",
    "    if(i == 1):\n",
    "        path_file = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/prepare_data/sentences_for_tokenizer.txt\"\n",
    "\n",
    "        with open(path_file, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            sen_txts = f.read()\n",
    "        sen_txts = sen_txts.split(\"\\n\")\n",
    "\n",
    "        #for i in [1000, 1500, 1800, 2000, 2500, 3000, 4000, 8000,16000, 32000]:# , \n",
    "        #    save_path = f\"tokenizer_sp_unique/sentence_piece_vocab_{i}/\"\n",
    "        #    sentencepiece_tk(data=sen_txts, vocab_size=i, save_path=save_path)\n",
    "        #    print(\"finished training with \", i, \" vocab size\")\n",
    "    \n",
    "    i=2\n",
    "    if(i == 2):\n",
    "        path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/txt_unique\"\n",
    "        list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "        \n",
    "        for i in [1000, 1500, 1800, 2000, 2500, 3000, 4000, 8000,16000, 32000]:# , \n",
    "            save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/tokenizer_WordPiece/WordPiece_vocab_{i}/\"\n",
    "            \n",
    "            wordpiece_tokenizer(files = list_of_files, vocab_size = i,\n",
    "                                save_path = save_path, min_frequency=2)\n",
    "            print(\"done with the first word piece\")\n",
    "    \n",
    "    i=3\n",
    "    if(i == 3):\n",
    "        path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\n",
    "        list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "        \n",
    "        for i in [1000, 1500, 1800, 2000, 2500, 3000, 4000, 8000,16000, 32000]:# , \n",
    "            save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/tokenizer_WordPiece_duplicated/WordPiece_vocab_{i}/\"\n",
    "            \n",
    "            wordpiece_tokenizer(files = list_of_files, vocab_size = i,\n",
    "                                save_path = save_path, min_frequency=2)\n",
    "            print(\"done with the first word piece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b35ec7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n",
      "about to train\n",
      "\n",
      "\n",
      "\n",
      "done training.. \n",
      "\n",
      "done with the first word piece\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a7c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/LanguageModelling/LanguageModelling/sentencepiece_duplicatedTk/ger-patho-bert-v1_sp_1000_epochs_20_batch_size_4/v1_sp_1000_epochs_20_batch_size_4.png\n"
     ]
    }
   ],
   "source": [
    "pp= f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/LanguageModelling/LanguageModelling/sentencepiece_duplicatedTk/ger-patho-bert-v1_sp_1000_epochs_20_batch_size_4/trainer_state.json\"\n",
    "title = \"v1_sp_1000_epochs_20_batch_size_4\"\n",
    "path_to_save_plot = \"/\".join(pp.split(\"/\")[:-1]) +\"/\"+title +\".png\"\n",
    "print(path_to_save_plot)\n",
    "    #plt.savefig(path_to_save_plot+\"/\"+title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b985b5e1",
   "metadata": {},
   "source": [
    "## clamp try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c7b0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f17ecf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-516ada5b713e5b01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/fb198/.cache/huggingface/datasets/json/default-516ada5b713e5b01/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927bae4e70cb4792a6b9fa9e1451a523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dadc96c61548ea945aa51d9f049cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/fb198/.cache/huggingface/datasets/json/default-516ada5b713e5b01/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "path_classi_data = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/DataNephroTexts/classification_data/data_files_synthetic_labeled_hf_dataset.json\"\n",
    "data = datasets.Dataset.from_json(path_classi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34c1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Klinische Angaben: multifokales Urothelkarzinom Harnblase pT1 G3 Harnleiterabsetzungsrand tumorfrei? Eingesandt wurde: 1 (HL-Absetzungsrand li) Zur Schnellschnittuntersuchung eingesandt wurde ein 18 x 13 x 7 cm messendes Harnblasenresektat mit anhängender Prostata von 6 x 5 x 5 cm. Im Bereich von Blasenboden/linker Seitenwand/Vorderwand ausgedehnte landkartenartige Ulzerationen von 7,5 x 3,8 cm ohne eindeutig abgrenzbaren Tumor. 1.1 SS Harnröhrenabsetzungsrand 1.2 Blasenpfeiler rechts 1.3 Blasenpfeiler links 1.4-1.9 Prostata rechts 1.10-1.15 Prostata links 1.16-1.25 Ulcusanteile, subtotal 1.16-1.31 Mapping Blasenschleimhaut Vorläufige Beurteilung gemäß der Gefrierschnittführung: Harnröhrenabsetzung tumorfrei. Auf die telefonische Schnellschnittbefunddurchsage darf verwiesen werden (Tel.nr. 8218, 11:18 Uhr). Beurteilung nach Paraffineinbettung: Harnblasenresektat mit an mehreren Stellen Residuen des vorbekannten, geringgradig differenzierten Urothelkarzinoms, welche beginnend bis in die Muskulatur infiltrieren und fokal auch auf die prostatische Harnröhre übergreifen. Daneben kräftige floride, ulzerierende, teils auch granuliende Entzündung neben Zeichen einer Urozystitis glandularis et cystica. Die Prostata mit überwiegend adenomyomatöser Hyperplasie. Die Resektion unter Einbeziehung von H/21/14942 (Harnleiterabsetzungen beidseits) in sano. Tumorstadium: pT2a L0 V0 Pn0 Resektionsstatus: R0 Malignitätsgrad: G3 Wir ergänzen noch eine PD-L1 Färbung und berichten erneut. Prof. Dr. med. A. Marx Prof. Dr. med. T. Gaiser Dr. med. D. Hirsch (0621/383-3026)',\n",
       " 'label': [1, 0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64849477",
   "metadata": {},
   "source": [
    "## train test BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beea303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the tokenizer and subword BPE trainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from data_prep_utils import get_txt_from_dir\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d49fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"[UNK]\" # token for unknown words\n",
    "special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg, vocab_size):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "        trainer = BpeTrainer(vocab_size = vocab_size, show_progress=True,\n",
    "                             continuing_subword_prefix = \"##\", special_tokens=special_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(vocab_size = vocab_size, show_progress=True,\n",
    "                                 continuing_subword_prefix = \"##\", unk_token=unk_token,\n",
    "                                 special_tokens=special_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=unk_token))\n",
    "        trainer = WordPieceTrainer(vocab_size = vocab_size, show_progress=True,\n",
    "                             continuing_subword_prefix = \"##\", special_tokens=special_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
    "        trainer = WordLevelTrainer(vocab_size = vocab_size, show_progress=True,\n",
    "                             continuing_subword_prefix = \"##\", special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer\n",
    "\n",
    "#‘WLV’ - Word Level Algorithm\n",
    "#‘WPC’ - WordPiece Algorithm\n",
    "#‘BPE’ - Byte Pair Encoding\n",
    "#‘UNI’ - Unigram\n",
    "def train_tokenizer(files, save_path, vocab_size, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg, vocab_size)\n",
    "    tokenizer.train(files, trainer) # training the tokenzier\n",
    "    \n",
    "    _save_path = f\"{save_path}/{alg}_{vocab_size}\"\n",
    "    if not os.path.exists(_save_path):\n",
    "        os.makedirs(_save_path)\n",
    "    \n",
    "    tokenizer.save(f\"{_save_path}/tokenizer.json\")\n",
    "    tokenizer_ = Tokenizer.from_file(f\"{_save_path}/tokenizer.json\")\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize(input_string, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string using the tokenizer provided.\n",
    "    \"\"\"\n",
    "    output = tokenizer.encode(input_string)\n",
    "    return output\n",
    "\n",
    "def fasttokenizer_wrapper(tokenizer_object, path_to_tk_dir, \n",
    "                          special_tokens, model_max_length=512):\n",
    "    \n",
    "    fast_tokenizer = PreTrainedTokenizerFast(\n",
    "                        tokenizer_object=tokenizer_object,\n",
    "                        name_or_path= path_to_tk_dir,\n",
    "                        model_max_length=model_max_length,\n",
    "                        special_tokens = special_tokens)\n",
    "\n",
    "    tokenizer_object.mask_token=\"[MASK]\"\n",
    "    tokenizer_object.unk_token =\"[UNK]\"\n",
    "    tokenizer_object.sep_token=\"[SEP]\"\n",
    "    tokenizer_object.pad_token=\"[PAD]\"\n",
    "    tokenizer_object.cls_token=\"[CLS]\"\n",
    "\n",
    "    fast_tokenizer.pad_token_id = tokenizer_object.token_to_id(\"[PAD]\")\n",
    "    fast_tokenizer.unk_token_id = tokenizer_object.token_to_id(\"[UNK]\")\n",
    "    fast_tokenizer.cls_token_id = tokenizer_object.token_to_id(\"[CLS]\")\n",
    "    fast_tokenizer.sep_token_id = tokenizer_object.token_to_id(\"[SEP]\")\n",
    "    fast_tokenizer.mask_token_id = tokenizer_object.token_to_id(\"[MASK]\")\n",
    "\n",
    "    if save_tokenizer:\n",
    "        fast_tokenizer.save_pretrained(path_to_tk_dir)\n",
    "        \n",
    "    return fast_tokenizer\n",
    "\n",
    "def main():\n",
    "    save_path_web = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web\"\n",
    "\n",
    "    path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\n",
    "    list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "    print(len(list_of_files))\n",
    "\n",
    "    input_string = [\"Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges,\"\n",
    "                   \" klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat.\"\n",
    "                   \" Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung\"\n",
    "                   \" eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung.\"\n",
    "                   \" 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit \"\n",
    "                   \"regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden\"\n",
    "                   \" Gewebematerial kein Anhalt für Malignität.\"\n",
    "                   \" Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091\"]\n",
    "    print(input_string)\n",
    "\n",
    "    tokens_dict = {}\n",
    "    for files in [list_of_files]:\n",
    "        #print(f\"========Using vocabulary from {files}=======\")\n",
    "        print(\"number of training file: \", len(files))\n",
    "        for alg in [ 'WPC']: #,'UNI','BPE']: #, \n",
    "            for vocab_size in [1000, 1200, 1500, 1800, 2000, 4000]:\n",
    "                trained_tokenizer = train_tokenizer(files, save_path_web, vocab_size, alg)\n",
    "\n",
    "                ## wrap it with transformers.PreTrainedTokenizerFast\n",
    "\n",
    "                fasttokenizer_wrapper(trained_tokenizer, f\"{save_path_web}/{alg}_{vocab_size}\",\n",
    "                                      special_tokens, model_max_length=512)\n",
    "\n",
    "                output = tokenize(input_string[0], trained_tokenizer)\n",
    "                tokens_dict[alg] = output.tokens\n",
    "                print(\"----\", alg, \"----\")\n",
    "                print(output.tokens, \"->\", len(output.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0e30df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161295\n",
      "['Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges, klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat. Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung. 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden Gewebematerial kein Anhalt für Malignität. Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091']\n",
      "number of training file:  161295\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_path_web = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web\"\n",
    "\n",
    "path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/txt_unique\"\n",
    "list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "print(len(list_of_files))\n",
    "\n",
    "input_string = [\"Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges,\"\n",
    "               \" klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat.\"\n",
    "               \" Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung\"\n",
    "               \" eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung.\"\n",
    "               \" 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit \"\n",
    "               \"regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden\"\n",
    "               \" Gewebematerial kein Anhalt für Malignität.\"\n",
    "               \" Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091\"]\n",
    "print(input_string)\n",
    "\n",
    "tokens_dict = {}\n",
    "for files in [list_of_files]:\n",
    "    #print(f\"========Using vocabulary from {files}=======\")\n",
    "    print(\"number of training file: \", len(files))\n",
    "    for alg in ['WPC']: #, \n",
    "        for vocab_size in [1000, 1200, 1500, 1800, 2000, 4000]:\n",
    "            trained_tokenizer = train_tokenizer(files, save_path_web, vocab_size, alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bfdbb7eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training file:  338991\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_660834/2519322570.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m## wrap it with transformers.PreTrainedTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             fasttokenizer_wrapper(trained_tokenizer, f\"{save_path}/{alg}_{vocab_size}_MAN\",\n\u001b[0m\u001b[1;32m     16\u001b[0m                                   special_tokens, model_max_length=512)\n",
      "\u001b[0;32m/tmp/ipykernel_660834/908018647.py\u001b[0m in \u001b[0;36mfasttokenizer_wrapper\u001b[0;34m(tokenizer_object, path_to_tk_dir, special_tokens, model_max_length)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mfast_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[MASK]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msave_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mfast_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_tk_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "path_to_txt = \"/home/fb198/BA/DataNephroTexts/train_tokenizer_data/numbers_punt_filter\"\n",
    "list_of_files = get_txt_from_dir(path_to_txt)\n",
    "save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/tokenizers/wordpiece_filtered_data\"\n",
    "\n",
    "tokens_dict = {}\n",
    "for files in [list_of_files]:\n",
    "    # print(f\"========Using vocabulary from {files}=======\")\n",
    "    print(\"number of training file: \", len(files))\n",
    "    for alg in ['WPC']:#, 'UNI', 'WPC']:  # ,\n",
    "        for vocab_size in [1000, 1200, 1500, 1800, 2000, 4000]:\n",
    "            trained_tokenizer = train_tokenizer(files, save_path, vocab_size, alg)\n",
    "\n",
    "            ## wrap it with transformers.PreTrainedTokenizerFast\n",
    "\n",
    "            fasttokenizer_wrapper(trained_tokenizer, f\"{save_path}/{alg}_{vocab_size}_MAN\",\n",
    "                                  special_tokens, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ee4ebbf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training file:  338991\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_txt = \"/home/fb198/BA/DataNephroTexts/train_tokenizer_data/numbers_punt_filter\"\n",
    "list_of_files = get_txt_from_dir(path_to_txt)\n",
    "save_path = f\"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/tokenizers/wordpiece_filtered_data\"\n",
    "\n",
    "tokens_dict = {}\n",
    "for files in [list_of_files]:\n",
    "    #print(f\"========Using vocabulary from {files}=======\")\n",
    "    print(\"number of training file: \", len(files))\n",
    "    for alg in ['WPC']: #, \n",
    "        for vocab_size in [1000, 1200, 1500, 1800, 2000, 4000]:\n",
    "            trained_tokenizer = train_tokenizer(files, save_path, vocab_size, alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13674f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_tokenizer = train_tokenizer(files, save_path_web, 1000, 'BPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cafa7f43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WPC ----\n",
      "['Klinische', 'Angaben', ':', 'A', '##d', '##ip', '##osit', '##as', 'r', '##b', '-', 'wb', 'Wir', 'erhielten', ':', '1', '(', 'Resekt', '##at', 'Magen', '):', 'Ein', 's', '##chl', '##a', '##uch', '##f', '##ör', '##m', '##ig', '##es', ',', 'kl', '##amm', '##ern', '##ah', '##t', '##vers', '##chlossen', '##es', ',', '1', '##6', ',', '8', 'cm', 'langes', 'und', 'max', '<UNK>', '3', ',', '8', 'cm', 'durchmessendes', 'Ma', '##g', '##ent', '##eil', '##resektat', '<UNK>', 'Auf', 'der', 'Schleimhaut', '##o', '##ber', '##fläche', 'sowie', 'den', 'Schnitt', '##fläch', '##en', 'kein', 'H', '##erd', '##be', '##fund', 'abgrenzbar', '<UNK>', '1', '<UNK>', '1', ':', 'Einbettung', 'eines', 'T', '##angential', '##s', '##chnitt', '##s', 'auf', 'die', 'unter', '##hal', '##b', 'der', 'K', '##l', '##amm', '##ern', '##ah', '##t', '##lieg', '##ende', 'Schleimhaut', '##absetzung', '<UNK>', '1', '<UNK>', '2', ':', 'Einbettung', 'eines', 'ex', '##emplar', '##ischen', 'z', '##entr', '##alen', 'Quers', '##chnitt', '##s', '<UNK>', 'Beurteilung', ':', 'Ma', '##g', '##ent', '##eil', '##resektat', 'mit', 'regel', '##rech', '##ter', ',', 'w', '##eit', '##ge', '##hend', 'e', '##ntzündung', '##s', '##frei', '##er', 'Schleimhaut', 'ohne', 'H', '##erd', '##be', '##fund', '<UNK>', 'Im', 'vorliegenden', 'Ge', '##we', '##b', '##em', '##aterial', 'kein', 'An', '##halt', 'für', 'M', '##al', '##ign', '##ität', '<UNK>', 'M', '##it', 'f', '##re', '##und', '##lichen', 'k', '##oll', '##eg', '##ial', '##en', 'G', '##rü', '##ß', '##en', 'Pro', '##f', '<UNK>', 'D', '##r', '<UNK>', 'm', '##ed', '<UNK>', 'A', '<UNK>', 'M', '##ar', '##x', 'D', '##r', '<UNK>', 'm', '##ed', '<UNK>', 'M', '<UNK>', 'H', '##ah', '##n', 'T', '##el', '<UNK>', '##:', '0', '##6', '##2', '##1', '/', '3', '##8', '##3', '-', '4', '##0', '##9', '##1'] -> 224\n"
     ]
    }
   ],
   "source": [
    "output = tokenize(input_string[0], trained_tokenizer)\n",
    "tokens_dict[alg] = output.tokens\n",
    "print(\"----\", alg, \"----\")\n",
    "print(output.tokens, \"->\", len(output.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "caba2fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x7feed58384f0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk_vocab_path = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/BPE_1200/tokenizer.json\"\n",
    "tokenizer = Tokenizer.from_file(tk_vocab_path)\n",
    "tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5363a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the tokenizer and subword BPE trainer\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "from data_prep_utils import get_txt_from_dir\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "unk_token = \"[UNK]\"  # token for unknown words\n",
    "special_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]  # special tokens\n",
    "\n",
    "\n",
    "def prepare_tokenizer_trainer(alg, vocab_size):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "        trainer = BpeTrainer(vocab_size=vocab_size, show_progress=True,\n",
    "                             continuing_subword_prefix=\"##\", special_tokens=special_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(vocab_size=vocab_size, show_progress=True,\n",
    "                                 unk_token=unk_token, special_tokens=special_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token=unk_token))\n",
    "        trainer = WordPieceTrainer(vocab_size=vocab_size, show_progress=True,\n",
    "                                   continuing_subword_prefix=\"##\", special_tokens=special_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n",
    "        trainer = WordLevelTrainer(vocab_size=vocab_size, show_progress=True,\n",
    "                                   continuing_subword_prefix=\"##\", special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer\n",
    "\n",
    "\n",
    "# ‘WLV’ - Word Level Algorithm\n",
    "# ‘WPC’ - WordPiece Algorithm\n",
    "# ‘BPE’ - Byte Pair Encoding\n",
    "# ‘UNI’ - Unigram\n",
    "def train_tokenizer(files, save_path, vocab_size, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg, vocab_size)\n",
    "    tokenizer.train(files, trainer)  # training the tokenzier\n",
    "\n",
    "    _save_path = f\"{save_path}/{alg}_{vocab_size}_dup\"\n",
    "    if not os.path.exists(_save_path):\n",
    "        os.makedirs(_save_path)\n",
    "\n",
    "    tokenizer.save(f\"{_save_path}/tokenizer.json\")\n",
    "    tokenizer_ = Tokenizer.from_file(f\"{_save_path}/tokenizer.json\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def tokenize(input_string, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string using the tokenizer provided.\n",
    "    \"\"\"\n",
    "    output = tokenizer.encode(input_string)\n",
    "    return output\n",
    "\n",
    "\n",
    "def fasttokenizer_wrapper(tokenizer_object, path_to_tk_dir,\n",
    "                          special_tokens, model_max_length=512, save_tokenizer=True):\n",
    "    fast_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer_object,\n",
    "        name_or_path=path_to_tk_dir,\n",
    "        model_max_length=model_max_length,\n",
    "        special_tokens=special_tokens)\n",
    "\n",
    "    tokenizer_object.mask_token = \"[MASK]\"\n",
    "    tokenizer_object.unk_token = \"[UNK]\"\n",
    "    tokenizer_object.sep_token = \"[SEP]\"\n",
    "    tokenizer_object.pad_token = \"[PAD]\"\n",
    "    tokenizer_object.cls_token = \"[CLS]\"\n",
    "\n",
    "    fast_tokenizer.pad_token_id = tokenizer_object.token_to_id(\"[PAD]\")\n",
    "    fast_tokenizer.unk_token_id = tokenizer_object.token_to_id(\"[UNK]\")\n",
    "    fast_tokenizer.cls_token_id = tokenizer_object.token_to_id(\"[CLS]\")\n",
    "    fast_tokenizer.sep_token_id = tokenizer_object.token_to_id(\"[SEP]\")\n",
    "    fast_tokenizer.mask_token_id = tokenizer_object.token_to_id(\"[MASK]\")\n",
    "\n",
    "    if save_tokenizer:\n",
    "        fast_tokenizer.save_pretrained(path_to_tk_dir)\n",
    "\n",
    "    return fast_tokenizer\n",
    "\n",
    "\n",
    "def main():\n",
    "    save_path_web = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web\"\n",
    "\n",
    "    path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/dup_texts\"\n",
    "    list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "    print(len(list_of_files))\n",
    "\n",
    "    input_string = [\"Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges,\"\n",
    "                    \" klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat.\"\n",
    "                    \" Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung\"\n",
    "                    \" eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung.\"\n",
    "                    \" 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit \"\n",
    "                    \"regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden\"\n",
    "                    \" Gewebematerial kein Anhalt für Malignität.\"\n",
    "                    \" Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091\"]\n",
    "    print(input_string)\n",
    "\n",
    "    tokens_dict = {}\n",
    "    for files in [list_of_files]:\n",
    "        # print(f\"========Using vocabulary from {files}=======\")\n",
    "        print(\"number of training file: \", len(files))\n",
    "        for alg in ['BPE']:  # ,'UNI', 'WPC'\n",
    "            for vocab_size in [1000, 2000, 4000]: #1200, 1500, 1800\n",
    "                trained_tokenizer = train_tokenizer(files, save_path_web, vocab_size, alg)\n",
    "\n",
    "                ## wrap it with transformers.PreTrainedTokenizerFast\n",
    "\n",
    "                fasttokenizer_wrapper(trained_tokenizer, f\"{save_path_web}/{alg}_{vocab_size}\",\n",
    "                                      special_tokens, model_max_length=512)\n",
    "\n",
    "                output = tokenize(input_string[0], trained_tokenizer)\n",
    "                tokens_dict[alg] = output.tokens\n",
    "                print(\"----\", alg, \"----\")\n",
    "                print(output.tokens, \"->\", len(output.tokens))\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4683112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4067ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161295\n",
      "['Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges, klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat. Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung. 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden Gewebematerial kein Anhalt für Malignität. Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091']\n"
     ]
    }
   ],
   "source": [
    "save_path_web = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web\"\n",
    "\n",
    "path_to_data_files = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/txt_unique\"\n",
    "list_of_files = get_txt_from_dir(path_to_data_files)\n",
    "print(len(list_of_files))\n",
    "\n",
    "input_string = [\"Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges,\"\n",
    "                \" klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat.\"\n",
    "                \" Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1: Einbettung\"\n",
    "                \" eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung.\"\n",
    "                \" 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung: Magenteilresektat mit \"\n",
    "                \"regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund. Im vorliegenden\"\n",
    "                \" Gewebematerial kein Anhalt für Malignität.\"\n",
    "                \" Mit freundlichen kollegialen Grüßen Prof.Dr.med.A.Marx Dr.med.M.Hahn Tel.: 0621/383-4091\"]\n",
    "print(input_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fb801cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if( i not in [3,4,5]):\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7db7608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training file:  161295\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_dict = {}\n",
    "for files in [list_of_files]:\n",
    "    # print(f\"========Using vocabulary from {files}=======\")\n",
    "    print(\"number of training file: \", len(files))\n",
    "    for alg in ['WPC']:#, 'UNI', 'WPC']:  # ,\n",
    "        for vocab_size in [1000]:#, 1200, 1500, 1800, 2000, 4000]:\n",
    "            trained_tokenizer = train_tokenizer(files, save_path_web, vocab_size, alg)\n",
    "\n",
    "            ## wrap it with transformers.PreTrainedTokenizerFast\n",
    "\n",
    "            fasttokenizer_wrapper(trained_tokenizer, f\"{save_path_web}/{alg}_{vocab_size}_MAN\",\n",
    "                                  special_tokens, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad34ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "trainer = BpeTrainer(vocab_size=1000, show_progress=True,\n",
    "continuing_subword_prefix=\"##\", special_tokens=special_tokens)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9137dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = Whitespace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(list_of_files, trainer)  # training the tokenzier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0443aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_save_path = f\"{save_path}/{alg}_1000_dup\"\n",
    "if not os.path.exists(_save_path):\n",
    "    os.makedirs(_save_path)\n",
    "\n",
    "tokenizer.save(f\"{_save_path}/tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293dee5b",
   "metadata": {},
   "source": [
    "## wordpiece example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59635479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Klinische Angaben: Adipositas rb-wb Wir erhielten: 1 (Resektat Magen): Ein schlauchförmiges,\"\n",
    "                \" klammernahtverschlossenes, 16,8 cm langes und max. 3,8 cm durchmessendes Magenteilresektat.\"\n",
    "                \" Auf der Schleimhautoberfläche sowie den Schnittflächen kein Herdbefund abgrenzbar. 1.1:Einbettung\"\n",
    "                \" eines Tangentialschnitts auf die unterhalb der Klammernahtliegende Schleimhautabsetzung.\"\n",
    "            \" 1.2: Einbettung eines exemplarischen zentralen Querschnitts. Beurteilung:\"\n",
    "            \". Im vorliegenden\"\n",
    "                \" Gewebematerial kein Anhalt für Malignität.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48d4c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = [ \n",
    "                \" Magenteilresektat mit \"\n",
    "                \"regelrechter, weitgehend entzündungsfreier Schleimhaut ohne Herdbefund.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81ecbc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Magen', '##teil', '##resektat', 'mit', 're', '##ge', '##l', '##rech', '##ter', ',', 'wei', '##t', '##ge', '##h', '##end', 'ent', '##zünd', '##ung', '##s', '##freie', '##r', 'Schleimhaut', 'ohne', 'H', '##erd', '##be', '##fund']\n"
     ]
    }
   ],
   "source": [
    "path_wp_1000 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/WPC_1000\"\n",
    "wp_1k = PreTrainedTokenizerFast.from_pretrained(path_wp_1000)\n",
    "wp_1k\n",
    "print(wp_1k.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e493a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f589bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Magen', '##teil', '##resektat', 'mit', 'regelrecht', '##er', ',', 'wei', '##t', '##ge', '##hend', 'ent', '##zünd', '##ungs', '##freie', '##r', 'Schleimhaut', 'ohne', 'H', '##erd', '##befund']\n"
     ]
    }
   ],
   "source": [
    "path_wp_1200 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/WPC_1200\"\n",
    "wp_1200 = PreTrainedTokenizerFast.from_pretrained(path_wp_1200)\n",
    "wp_1200\n",
    "print(wp_1200.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4abb2fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Magenteilresektat', 'mit', 'regelrechter', ',', 'weitgehend', 'entzündungsfreie', '##r', 'Schleimhaut', 'ohne', 'Herdbefund']\n"
     ]
    }
   ],
   "source": [
    "path_wp_4000 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/WPC_4000\"\n",
    "wp_4k = PreTrainedTokenizerFast.from_pretrained(path_wp_4000)\n",
    "wp_4k\n",
    "print(wp_4k.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b68314a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġ', 'M', 'ag', 'en', 'te', 'i', 'l', 'r', 'e', 's', 'e', 'k', 't', 'at', 'Ġ', 'm', 'i', 't', 'Ġ', 'r', 'e', 'g', 'e', 'l', 'r', 'e', 'c', 'h', 'ter', ',', 'Ġ', 'w', 'e', 'i', 't', 'g', 'e', 'h', 'en', 'd', 'Ġ', 'en', 't', 'z', 'Ã', '¼', 'n', 'd', 'u', 'n', 'g', 's', 'f', 'r', 'e', 'i', 'er', 'Ġ', 'S', 'c', 'h', 'l', 'e', 'i', 'm', 'h', 'a', 'u', 't', 'Ġ', 'o', 'h', 'n', 'e', 'Ġ', 'H', 'er', 'd', 'b', 'e', 'f', 'u', 'n', 'd']\n"
     ]
    }
   ],
   "source": [
    "path_bpe_drop_1200 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/BPE_tokenizer_dropout/1_BPE_1_batch_size_8_vocab_4000_\"\n",
    "bpe_drop_1200 = PreTrainedTokenizerFast.from_pretrained(path_bpe_drop_1200)\n",
    "bpe_drop_1200\n",
    "print(bpe_drop_1200.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "583880f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ma', '##g', '##ent', '##eil', '##resektat', 'mit', 'regel', '##rech', '##ter', ',', 'w', '##eit', '##ge', '##hend', 'e', '##ntzündung', '##s', '##frei', '##er', 'Schleimhaut', 'ohne', 'H', '##erd', '##befund']\n"
     ]
    }
   ],
   "source": [
    "path_bpe_drop_1200 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/BPE_1200\"\n",
    "bpe_drop_1200 = PreTrainedTokenizerFast.from_pretrained(path_bpe_drop_1200)\n",
    "bpe_drop_1200\n",
    "print(bpe_drop_1200.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41b9ba79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Magen', '##teil', '##resektat', 'mit', 'r', '##egelre', '##ch', '##ter', ',', 'w', '##eit', '##ge', '##h', '##end', 'e', '##ntzündung', '##s', '##frei', '##er', 'Schleimhaut', 'ohne', 'H', '##erd', '##be', '##fund']\n"
     ]
    }
   ],
   "source": [
    "path_bpe_drop_1500 = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/jupyter_stuff/tokenizers_vs4_12_10/test_toenizer_from_web/BPE_1000\"\n",
    "bpe_drop_1500 = PreTrainedTokenizerFast.from_pretrained(path_bpe_drop_1500)\n",
    "bpe_drop_1500\n",
    "print(bpe_drop_1500.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af009631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Magen', 'teil', 'resektat', '▁mit', '▁regel', 'rech', 'ter', ',', '▁w', 'eit', 'ge', 'h', 'end', '▁en', 'tzündung', 's', 'frei', 'er', '▁Schleimhaut', '▁ohne', '▁H', 'erd', 'be', 'fund', '.']\n"
     ]
    }
   ],
   "source": [
    "sp_path = \"/home/fb198/BA/nlp-in-diagnostic-texts-from-nephropathology-master/LanguageModelling/LanguageModelling/mlm_evaluation_2/bert-1_sp_1_batch_size_8\"\n",
    "sp_tk = PreTrainedTokenizerFast.from_pretrained(sp_path)\n",
    "print(sp_tk.tokenize(input_string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517d2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
